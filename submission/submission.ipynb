{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import glob\r\n",
                "import joblib\r\n",
                "import lightgbm as lgb\r\n",
                "import numpy as np\r\n",
                "import os\r\n",
                "import pandas as pd\r\n",
                "from abc import ABCMeta, abstractmethod\r\n",
                "from sklearn.preprocessing import MinMaxScaler\r\n",
                "from sklearn.model_selection import KFold\r\n",
                "from tqdm import tqdm\r\n",
                "from typing import Callable, List, Tuple, Union, Optional"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "DATA_DIR = '../input/optiver-realized-volatility-prediction/'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "class CreateFeatures:\r\n",
                "    '''\r\n",
                "    features\r\n",
                "    ========\r\n",
                "        - stock_id: [categorical]\r\n",
                "        - past_vol: volatility during the feature bucket of each (stock_id, time_id)\r\n",
                "        - past_vol2: past volatility calculated with WAP2\r\n",
                "        - updown: Compare average WAP of first 10 seconds_in_bucket and last 10 in the feature bucket.\r\n",
                "                  +1 if the first 10 < the last 10, -1 otherwise.\r\n",
                "        - price_spread1: mean of ask_price1 - bid_price1\r\n",
                "        - price_spread2: mean of ask_price2 - bid_price2\r\n",
                "        - price_spread_bid: mean of bid_price1 - bid_price2\r\n",
                "        - price_spread_ask: mean of ask_price1 - ask_price2\r\n",
                "        - size_total_bid: mean of bid_size1 + bid_size2\r\n",
                "        - size_total_ask: mean of ask_size1 + ask_size2\r\n",
                "        - size_total1: mean of bid_size1 + ask_size1\r\n",
                "        - size_total2: mean of bid_size2 + ask_size2\r\n",
                "        - size_total3: mean of bid_size1 + bid_size2 + ask_size1 + ask_size2\r\n",
                "        - size_spread1: mean of bid_size1 - ask_size1\r\n",
                "        - size_spread2: mean of bid_size2 - ask_size2\r\n",
                "        - size_spread3: mean of bid_size1 + bid_size2 - ask_size1 - ask_size2\r\n",
                "    '''\r\n",
                "\r\n",
                "    def __init__(self, rerun: bool) -> None:\r\n",
                "        self.rerun = rerun\r\n",
                "        # {feature_name: [aggregation method, aggregation target column in book]}\r\n",
                "        self.agg_method_target = {\r\n",
                "                                'stock_id': [None, None],\r\n",
                "                                'time_id': [None, None],\r\n",
                "                                'vol': [None, 'WAP'],\r\n",
                "                                'vol2': [None, 'WAP2'],\r\n",
                "                                'updown': [None, 'WAP'],\r\n",
                "                                'price_spread1': [np.mean, 'price_spread1'],\r\n",
                "                                'price_spread2': [np.mean, 'price_spread2'],\r\n",
                "                                'price_spread_bid': [np.mean, 'price_spread_bid'],\r\n",
                "                                'price_spread_ask': [np.mean, 'price_spread_ask'],\r\n",
                "                                'size_total_bid': [np.mean, 'size_total_bid'],\r\n",
                "                                'size_total_ask': [np.mean, 'size_total_ask'],\r\n",
                "                                'size_total1': [np.mean, 'size_total1'],\r\n",
                "                                'size_total2': [np.mean, 'size_total2'],\r\n",
                "                                'size_total3': [np.mean, 'size_total3'],\r\n",
                "                                'size_spread1': [np.mean, 'size_spread1'],\r\n",
                "                                'size_spread2': [np.mean, 'size_spread2'],\r\n",
                "                                'size_spread3': [np.mean, 'size_spread3']\r\n",
                "                                }\r\n",
                "\r\n",
                "    def create_features(self, train_test: str) -> Tuple[pd.DataFrame, List[str]]:\r\n",
                "        ''' creates DataFrame of scaled features of all (stock_id, time_id)\r\n",
                "            the DataFrame will be saved as a csv file\r\n",
                "\r\n",
                "        :param train_test: either 'train' or 'test'\r\n",
                "        :return: DataFrame of scaled features and list of feature names\r\n",
                "        '''\r\n",
                "        if not self.rerun:\r\n",
                "            # just load an already-prepared feature data\r\n",
                "            df_feature = pd.read_csv(f'features_{train_test}.csv')\r\n",
                "            feature_names = list(set(df_feature.columns) - set('time_id'))\r\n",
                "        else:\r\n",
                "            print(f'Creating features for {train_test}ing...')\r\n",
                "            book_list = glob.glob(DATA_DIR + f'book_{train_test}.parquet/*')\r\n",
                "            # trade_list = glob.glob(DATA_DIR + f'trade_{train_test}.parquet/*')\r\n",
                "            self.agg_method_target['vol'][0] = self._calc_vol\r\n",
                "            self.agg_method_target['vol2'][0] = self._calc_vol\r\n",
                "            self.agg_method_target['updown'][0] = self._calc_updown\r\n",
                "            \r\n",
                "            feature_dict = {feature_name: [] for feature_name in self.agg_method_target}\r\n",
                "            for book_path in tqdm(book_list):\r\n",
                "                # for each stock_id\r\n",
                "                book = pd.read_parquet(book_path)\r\n",
                "                book = self._add_wap_and_etc_to_book(book)\r\n",
                "\r\n",
                "                feature_dict_of_stock_id = {}\r\n",
                "                for feature_name in self.agg_method_target:\r\n",
                "                    if feature_name == 'stock_id' or feature_name == 'time_id':\r\n",
                "                        pass\r\n",
                "                    else:\r\n",
                "                        agg_method = self.agg_method_target[feature_name][0]\r\n",
                "                        agg_target = self.agg_method_target[feature_name][1]\r\n",
                "                        feature_dict_of_stock_id[feature_name] = book.groupby('time_id')[agg_target].agg(agg_method)\r\n",
                "                        feature_dict[feature_name] += list(feature_dict_of_stock_id[feature_name])\r\n",
                "\r\n",
                "                feature_dict['stock_id'] += [int(book_path.split('=')[1])] * feature_dict_of_stock_id['vol'].shape[0]\r\n",
                "                feature_dict['time_id'] += list(feature_dict_of_stock_id['vol'].index)\r\n",
                "\r\n",
                "            df_feature = pd.DataFrame({feature_name: feature_dict[feature_name] for feature_name in self.agg_method_target})\r\n",
                "\r\n",
                "            scaler = MinMaxScaler()\r\n",
                "            feature_names = list(set(list(df_feature.columns)) - set(['time_id']))\r\n",
                "            feature_names_num = list(set(list(feature_names)) - set(['stock_id']))\r\n",
                "            df_feature[feature_names_num] = pd.DataFrame(scaler.fit_transform(df_feature[feature_names_num]),\r\n",
                "                                                     columns=feature_names_num)\r\n",
                "            df_feature.to_csv(f'features_{train_test}.csv', encoding='utf-8-sig', index=False)\r\n",
                "            print(f'Finish creating features for {train_test}ing!')\r\n",
                "        return df_feature, feature_names\r\n",
                "\r\n",
                "    # all the functions below are helper functions\r\n",
                "    def _add_wap_and_etc_to_book(self, book: pd.DataFrame) -> pd.DataFrame:\r\n",
                "        ''' calculates WAP and etc. for a stock for each second in bucket\r\n",
                "\r\n",
                "        :param: pd.DataFrame of a book\r\n",
                "        :return: pd.DataFrame of a book with new columns\r\n",
                "        '''\r\n",
                "        book['WAP'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) \\\r\n",
                "                           / (book['bid_size1'] + book['ask_size1'])\r\n",
                "        book['WAP2'] = (book['bid_price2'] * book['ask_size2'] + book['ask_price2'] * book['bid_size2']) \\\r\n",
                "                       / (book['bid_size2'] + book['ask_size2'])\r\n",
                "        book['price_spread1'] = book['ask_price1'] - book['bid_price1']\r\n",
                "        book['price_spread2'] = book['ask_price2'] - book['bid_price2']\r\n",
                "        book['price_spread_bid'] = book['bid_price1'] - book['bid_price2']\r\n",
                "        book['price_spread_ask'] = book['ask_price1'] - book['ask_price2']\r\n",
                "        book['size_total_bid'] = book['bid_size1'] + book['bid_size2']\r\n",
                "        book['size_total_ask'] = book['ask_size1'] + book['ask_size2']\r\n",
                "        book['size_total1'] = book['bid_size1'] + book['ask_size1']\r\n",
                "        book['size_total2'] = book['bid_size2'] + book['ask_size2']\r\n",
                "        book['size_total3'] = book['bid_size1'] + book['bid_size2'] + book['ask_size1'] + book['ask_size2']\r\n",
                "        book['size_spread1'] = book['bid_size1'] - book['ask_size1']\r\n",
                "        book['size_spread2'] = book['bid_size2'] - book['ask_size2']\r\n",
                "        book['size_spread3'] = book['bid_size1'] + book['bid_size2'] - book['ask_size1'] - book['ask_size2']\r\n",
                "\r\n",
                "        return book\r\n",
                "        \r\n",
                "    def _calc_vol(self, wap_series: pd.Series) -> float:\r\n",
                "        ''' calculates volatility of a srtock during a time_id\r\n",
                "\r\n",
                "        :param wap_series: series of weighted average price\r\n",
                "        :return: volatility of a stock in the first 10 min\r\n",
                "        '''\r\n",
                "        # calculate log return log(S_{k}/S_{k-1}) for each k\r\n",
                "        # where S_{k} is the price of the stock S at time k\r\n",
                "        # and k is the index of the input list.\r\n",
                "        list_log_return = np.log(wap_series).diff()\r\n",
                "        return np.sqrt(np.sum(list_log_return**2))\r\n",
                "\r\n",
                "    def _calc_updown(self, wap_series: pd.Series) -> float:\r\n",
                "        ''' judges if WAP went up or down during the time_id\r\n",
                "\r\n",
                "        :return: 1 when up, -1 when down, or 0 when neutral\r\n",
                "        '''\r\n",
                "        list_wap = list(wap_series)\r\n",
                "        diff = np.mean(list_wap[-10:]) - np.mean(list_wap[:10])\r\n",
                "        return np.sign(diff)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "class Model(metaclass=ABCMeta):\r\n",
                "\r\n",
                "    def __init__(self, run_fold_name: str, feature_names: List[str],\r\n",
                "                 params: dict) -> None:\r\n",
                "        ''' Constructor\r\n",
                "\r\n",
                "        :param feature_names: list of feature names to specify columns of feature dataframe\r\n",
                "        :param params: hyper parameters\r\n",
                "        '''\r\n",
                "        self.run_fold_name = run_fold_name\r\n",
                "        self.feature_names = feature_names\r\n",
                "        self.params = params\r\n",
                "        self.model = None\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def train(self, X_train: pd.DataFrame, y_train: pd.Series,\r\n",
                "              X_valid: Optional[pd.DataFrame] = None,\r\n",
                "              y_valid: Optional[pd.Series] = None\r\n",
                "              ) -> None:\r\n",
                "        ''' trains a model\r\n",
                "\r\n",
                "        :param X_train: features of training data\r\n",
                "        :param y_train: targets of training data\r\n",
                "        :param X_valid: features of validation data\r\n",
                "        :param y_valid: targets of validation data\r\n",
                "        '''\r\n",
                "        pass\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def predict(self, X: pd.DataFrame) -> np.array:\r\n",
                "        ''' returns prediction output from a learned model\r\n",
                "\r\n",
                "        :param X: features of test data or validation data\r\n",
                "        :return: predicted value\r\n",
                "        '''\r\n",
                "        pass\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def save_model(self) -> None:\r\n",
                "        ''' saves a model '''\r\n",
                "        pass\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def load_model(self) -> None:\r\n",
                "        ''' loads a model '''\r\n",
                "        pass"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "class ModelLGB(Model):\r\n",
                "\r\n",
                "\r\n",
                "    def train(self, X_train, y_train, X_valid=None, y_valid=None):\r\n",
                "\r\n",
                "        params = self.params\r\n",
                "        if X_valid is not None:\r\n",
                "            # weights to change feval from RMSE to RMSPE\r\n",
                "            # idea from: https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/250324\r\n",
                "            w_train = 1/np.square(y_train)\r\n",
                "            w_valid = 1/np.square(y_valid)\r\n",
                "            lgb_train = lgb.Dataset(X_train, y_train, weight=w_train)\r\n",
                "            lgb_eval = lgb.Dataset(X_valid, y_valid, weight=w_valid)\r\n",
                "            self.model = lgb.train(params,\r\n",
                "                                   lgb_train,\r\n",
                "                                   num_boost_round=100,\r\n",
                "                                   valid_sets=lgb_eval,\r\n",
                "                                   feval=rmspe,\r\n",
                "                                   verbose_eval=50,\r\n",
                "                                   categorical_feature=['stock_id']\r\n",
                "                                  )\r\n",
                "        else:\r\n",
                "            w_train = 1/np.square(y_train)\r\n",
                "            lgb_train = lgb.Dataset(X_train, y_train, weight=w_train)\r\n",
                "            self.model = lgb.train(params,\r\n",
                "                                   lgb_train,\r\n",
                "                                   num_boost_round=100,\r\n",
                "                                   feval=rmspe,\r\n",
                "                                   categorical_feature=['stock_id']\r\n",
                "                           )\r\n",
                "\r\n",
                "    def predict(self, X_test):\r\n",
                "        return self.model.predict(X_test)\r\n",
                "\r\n",
                "    def save_model(self):\r\n",
                "        self.model.save_model('model.txt')\r\n",
                "\r\n",
                "    def load_model(self):\r\n",
                "        self.model = lgb.Booster(model_file='model.txt')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# delf-defiend evaluation metric function\r\n",
                "def rmspe(y_true: np.array, y_pred: np.array):\r\n",
                "    ''' self-defined eval metric\r\n",
                "        Root Mean Squared Percentage Error\r\n",
                "\r\n",
                "    :return: name: str, eval_result: float, is_higher_better: bool\r\n",
                "    '''\r\n",
                "    if type(y_pred) == lgb.basic.Dataset:\r\n",
                "        y_pred = y_pred.get_label()\r\n",
                "    rmspe = (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\r\n",
                "    return 'RMSPE', rmspe, False"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "class Util:\r\n",
                "    @classmethod\r\n",
                "    def dump(cls, value, path):\r\n",
                "        os.makedirs(os.path.dirname(path), exist_ok=True)\r\n",
                "        joblib.dump(value, path, compress=True)\r\n",
                "\r\n",
                "    @classmethod\r\n",
                "    def load(cls, path):\r\n",
                "        return joblib.load(path)\r\n",
                "\r\n",
                "    @classmethod\r\n",
                "    def submission(cls, df_pred: pd.DataFrame) -> None:\r\n",
                "        row_id = df_pred['stock_id'].apply(str) + '-' + df_pred['time_id'].apply(str)\r\n",
                "        df_submission = pd.DataFrame({'row_id': row_id, 'target':df_pred['target']})\r\n",
                "        df_submission.to_csv('submission.csv', index=False)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "class Runner:\r\n",
                "\r\n",
                "    def __init__(self, run_name: str, model_cls: Callable[[str, dict], Model],\r\n",
                "                 feature_names: List[str], df_features_train: pd.DataFrame,\r\n",
                "                 df_features_test: pd.DataFrame, params: dict):\r\n",
                "        ''' Constructor\r\n",
                "\r\n",
                "        :param run_name: name of run\r\n",
                "        :param model_cls: class of model\r\n",
                "        :param feature_names: list of feature names\r\n",
                "        :param params: hyper parameters\r\n",
                "        '''\r\n",
                "        self.run_name = run_name\r\n",
                "        self.model_cls = model_cls\r\n",
                "        self.feature_names = feature_names\r\n",
                "        self.X_train_all = df_features_train\r\n",
                "        self.X_test = df_features_test\r\n",
                "        self.params = params\r\n",
                "        self.n_fold = 4\r\n",
                "\r\n",
                "    def _train_fold(self, i_fold: Union[int, str]) -> Tuple[\r\n",
                "                    Model, Optional[np.array],\r\n",
                "                    Optional[np.array], Optional[float]]:\r\n",
                "        ''' specifies number of fold for cv then learns & evaluates\r\n",
                "\r\n",
                "        :param i_fold: number of fold ('all' for all)\r\n",
                "        :return: a tuple of instance of model, index of record,\r\n",
                "                 predicted value, and evaluation score\r\n",
                "                 (returns only model if i_fold=='all')\r\n",
                "        '''\r\n",
                "        # load train data\r\n",
                "        y_train_all = self._load_y_train()\r\n",
                "        self.X_train_all['stock_id'] = self.X_train_all['stock_id'].astype('int64')\r\n",
                "        self.X_train_all['time_id'] = self.X_train_all['time_id'].astype('int64')\r\n",
                "        Xy_train_all = pd.merge(self.X_train_all, y_train_all,\r\n",
                "                                left_on=['stock_id','time_id'], right_on=['stock_id','time_id'],\r\n",
                "                                how='inner')\r\n",
                "        X_train_all = Xy_train_all[self.feature_names]\r\n",
                "        y_train_all = Xy_train_all['target']\r\n",
                "\r\n",
                "        validation = i_fold != 'all'\r\n",
                "        if validation:\r\n",
                "            # split data into training and validation\r\n",
                "            idx_train, idx_valid = self._load_index_fold(i_fold)\r\n",
                "            X_train = X_train_all.iloc[idx_train]\r\n",
                "            y_train = y_train_all.iloc[idx_train]\r\n",
                "            X_valid = X_train_all.iloc[idx_valid]\r\n",
                "            y_valid = y_train_all.iloc[idx_valid]\r\n",
                "\r\n",
                "            # execute learning\r\n",
                "            model = self._build_model(i_fold)\r\n",
                "            model.train(X_train, y_train, X_valid, y_valid)\r\n",
                "\r\n",
                "            # prediction and evaluation with validation data\r\n",
                "            pred_valid = model.predict(X_valid)\r\n",
                "            _, score, _ = rmspe(y_true=y_valid, y_pred=pred_valid)\r\n",
                "\r\n",
                "            # return model, index, prediction, and score\r\n",
                "            return model, idx_valid, pred_valid, score\r\n",
                "        else:\r\n",
                "            # learining with all data\r\n",
                "            model = self._build_model(i_fold)\r\n",
                "            model.train(X_train_all, y_train_all)\r\n",
                "\r\n",
                "            # return model\r\n",
                "            return model, None, None, None\r\n",
                "\r\n",
                "    def run_train_cv(self) -> None:\r\n",
                "        ''' learns and evaluates by CV\r\n",
                "\r\n",
                "        learns, evaluates, and saves models and scores of each fold\r\n",
                "        '''\r\n",
                "        scores = []\r\n",
                "        idxes_valid = []\r\n",
                "        preds = []\r\n",
                "\r\n",
                "        # learning for each fold\r\n",
                "        for i_fold in range(self.n_fold):\r\n",
                "            model, idx_valid, pred_valid, score = self._train_fold(i_fold)\r\n",
                "\r\n",
                "            # hold result\r\n",
                "            idxes_valid.append(idx_valid)\r\n",
                "            scores.append(score)\r\n",
                "            preds.append(pred_valid)\r\n",
                "\r\n",
                "            # save model\r\n",
                "            model.save_model()\r\n",
                "        print(f'Mean score of the folds: {np.mean(scores)}')\r\n",
                "\r\n",
                "    def run_predict_cv(self) -> pd.DataFrame:\r\n",
                "        ''' predicts for test data with the mean of\r\n",
                "            each fold's model learned through CV\r\n",
                "            \r\n",
                "            :return: predicted target as the mean of folds\r\n",
                "        '''\r\n",
                "        preds = []\r\n",
                "        # prediction for each fold's model\r\n",
                "        for i_fold in range(self.n_fold):\r\n",
                "            model = self._build_model(i_fold)\r\n",
                "            model.load_model()\r\n",
                "            pred = model.predict(self.X_test[self.feature_names])\r\n",
                "            preds.append(pred)\r\n",
                "\r\n",
                "        # mean of the prediction values\r\n",
                "        pred_mean = np.mean(preds, axis=0)\r\n",
                "        df_pred = self.X_test[self.X_test.columns[:2]]\r\n",
                "        df_pred.loc[:, 'target'] = pred_mean\r\n",
                "        return df_pred\r\n",
                "\r\n",
                "    def run_train_all(self) -> None:\r\n",
                "        ''' learns with all the training data and save the model '''\r\n",
                "        # learning\r\n",
                "        i_fold = 'all'\r\n",
                "        model, _, _, _ = self._train_fold(i_fold)\r\n",
                "        model.save_model()\r\n",
                "\r\n",
                "    def run_predict_all(self) -> pd.DataFrame:\r\n",
                "        ''' predicts for test data with the model learned with all the training data\r\n",
                "\r\n",
                "        :return: predicted target\r\n",
                "        '''\r\n",
                "        \r\n",
                "        # predict with the mdoel learned with all the learning data\r\n",
                "        i_fold = 'all'\r\n",
                "        model = self._build_model(i_fold)\r\n",
                "        model.load_model()\r\n",
                "        pred = model.predict(self.X_test[self.feature_names])\r\n",
                "        df_pred = self.X_test[self.X_test.columns[:2]]\r\n",
                "        df_pred['target'] = pred\r\n",
                "        return df_pred\r\n",
                "\r\n",
                "    def _build_model(self, i_fold: Union[int, str]) -> Model:\r\n",
                "        ''' builds a model with a specified fold for cv\r\n",
                "\r\n",
                "        :param i_fold: number of fold\r\n",
                "        :return: instance of model\r\n",
                "        '''\r\n",
                "        # build a model with run name, fold, and class of model\r\n",
                "        run_fold_name = f'{self.run_name}-{i_fold}'\r\n",
                "        return self.model_cls(run_fold_name, self.feature_names, self.params)\r\n",
                "\r\n",
                "    def _load_y_train(self) -> pd.DataFrame:\r\n",
                "        ''' loads target of train data; ['stock_id', 'time_id', 'target']\r\n",
                "\r\n",
                "        :return: target dataframe of train data\r\n",
                "        '''\r\n",
                "        return pd.read_csv(DATA_DIR + 'train.csv')\r\n",
                "\r\n",
                "    def _load_index_fold(self, i_fold: int) -> np.array:\r\n",
                "        ''' returns the record index in response to the fold specified for cv\r\n",
                "\r\n",
                "        :param i_fold: number of the fold\r\n",
                "        :return: record index for the fold\r\n",
                "        '''\r\n",
                "        # return index to split data for learning and validation\r\n",
                "        y_train = self._load_y_train()\r\n",
                "        x_dummy = np.zeros(len(y_train))\r\n",
                "        skf = KFold(n_splits=self.n_fold, shuffle=True, random_state=31)\r\n",
                "        return list(skf.split(x_dummy, y_train))[i_fold]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## main"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "params_lgb = {\r\n",
                "            'boosting_type': 'gbdt',\r\n",
                "            'num_leaves': 100,\r\n",
                "            'learning_rate': 0.05,\r\n",
                "            'n_estimators': 1000,\r\n",
                "            'metric': 'rmse',\r\n",
                "            'early_stopping_rounds': 100\r\n",
                "            }\r\n",
                "\r\n",
                "# rerun=False if just using saved features works\r\n",
                "cf = CreateFeatures(rerun=True)\r\n",
                "df_features_train, feature_names = cf.create_features('train')\r\n",
                "df_features_test, _ = cf.create_features('test')\r\n",
                "\r\n",
                "# learning and prediction by lightGBM and create submission file\r\n",
                "run_name = 'lgb'\r\n",
                "runner = Runner(run_name=run_name,\r\n",
                "                model_cls=ModelLGB,\r\n",
                "                feature_names=feature_names,\r\n",
                "                df_features_train=df_features_train,\r\n",
                "                df_features_test=df_features_test,\r\n",
                "                params=params_lgb)\r\n",
                "runner.run_train_cv()\r\n",
                "pred = runner.run_predict_cv()\r\n",
                "Util.submission(pred)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "  0%|          | 0/112 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Creating features for training...\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 112/112 [11:03<00:00,  5.92s/it]\n",
                        "100%|██████████| 1/1 [00:00<00:00, 20.00it/s]"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Finish creating features for training!\n",
                        "Creating features for testing...\n",
                        "Finish creating features for testing!\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098948 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 3684\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 16\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001803\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000543827\tvalid_0's RMSPE: 0.380046\n",
                        "[100]\tvalid_0's rmse: 0.000535151\tvalid_0's RMSPE: 0.361781\n",
                        "[150]\tvalid_0's rmse: 0.000534749\tvalid_0's RMSPE: 0.355537\n",
                        "[200]\tvalid_0's rmse: 0.000535292\tvalid_0's RMSPE: 0.353013\n",
                        "[250]\tvalid_0's rmse: 0.000535791\tvalid_0's RMSPE: 0.351218\n",
                        "Early stopping, best iteration is:\n",
                        "[161]\tvalid_0's rmse: 0.000534605\tvalid_0's RMSPE: 0.355014\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069347 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 3684\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 16\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001801\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.00053513\tvalid_0's RMSPE: 0.37249\n",
                        "[100]\tvalid_0's rmse: 0.000525305\tvalid_0's RMSPE: 0.352498\n",
                        "[150]\tvalid_0's rmse: 0.000523817\tvalid_0's RMSPE: 0.348956\n",
                        "[200]\tvalid_0's rmse: 0.000523273\tvalid_0's RMSPE: 0.346179\n",
                        "[250]\tvalid_0's rmse: 0.000523173\tvalid_0's RMSPE: 0.344354\n",
                        "[300]\tvalid_0's rmse: 0.00052323\tvalid_0's RMSPE: 0.342755\n",
                        "Early stopping, best iteration is:\n",
                        "[218]\tvalid_0's rmse: 0.000523087\tvalid_0's RMSPE: 0.345355\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090730 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 3684\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 16\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001800\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000544708\tvalid_0's RMSPE: 0.375441\n",
                        "[100]\tvalid_0's rmse: 0.000537773\tvalid_0's RMSPE: 0.354413\n",
                        "[150]\tvalid_0's rmse: 0.000537657\tvalid_0's RMSPE: 0.350194\n",
                        "[200]\tvalid_0's rmse: 0.000539015\tvalid_0's RMSPE: 0.347649\n",
                        "Early stopping, best iteration is:\n",
                        "[136]\tvalid_0's rmse: 0.0005372\tvalid_0's RMSPE: 0.351213\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065569 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 3684\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 16\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001797\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000539602\tvalid_0's RMSPE: 0.374903\n",
                        "[100]\tvalid_0's rmse: 0.000529953\tvalid_0's RMSPE: 0.354999\n",
                        "[150]\tvalid_0's rmse: 0.000529051\tvalid_0's RMSPE: 0.350539\n",
                        "[200]\tvalid_0's rmse: 0.000528815\tvalid_0's RMSPE: 0.348501\n",
                        "[250]\tvalid_0's rmse: 0.000528539\tvalid_0's RMSPE: 0.346601\n",
                        "[300]\tvalid_0's rmse: 0.000528444\tvalid_0's RMSPE: 0.345295\n",
                        "[350]\tvalid_0's rmse: 0.000528418\tvalid_0's RMSPE: 0.344135\n",
                        "[400]\tvalid_0's rmse: 0.000528568\tvalid_0's RMSPE: 0.343095\n",
                        "Early stopping, best iteration is:\n",
                        "[342]\tvalid_0's rmse: 0.000528331\tvalid_0's RMSPE: 0.344261\n",
                        "Mean score of the folds: 0.24566350475139093\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  self.obj[key] = value\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  self._setitem_single_column(ilocs[0], value, pi)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('mlenv': conda)"
        },
        "interpreter": {
            "hash": "708ee50cc1f420053506a448dda1816f977733ceebd2b39e67d8a3a227913ecf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}