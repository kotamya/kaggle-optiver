{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import glob\r\n",
                "import joblib\r\n",
                "import lightgbm as lgb\r\n",
                "import numpy as np\r\n",
                "import os\r\n",
                "import pandas as pd\r\n",
                "from abc import ABCMeta, abstractmethod\r\n",
                "from sklearn.preprocessing import MinMaxScaler\r\n",
                "from sklearn.model_selection import KFold\r\n",
                "from tqdm import tqdm\r\n",
                "from typing import Callable, List, Tuple, Union, Optional, Iterable"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "    '''\r\n",
                "    features\r\n",
                "    ========\r\n",
                "        - stock_id: [categorical]\r\n",
                "        - past_vol: volatility during the feature bucket of each (stock_id, time_id)\r\n",
                "        - past_vol2: past volatility calculated with WAP2\r\n",
                "        - updown: Compare average WAP of first 10 seconds_in_bucket and last 10 in the feature bucket.\r\n",
                "                  +1 if the first 10 < the last 10, -1 otherwise.\r\n",
                "        - price_spread1: mean of ask_price1 - bid_price1\r\n",
                "        - price_spread2: mean of ask_price2 - bid_price2\r\n",
                "        - price_spread_bid: mean of bid_price1 - bid_price2\r\n",
                "        - price_spread_ask: mean of ask_price1 - ask_price2\r\n",
                "        - size_total_bid: mean of bid_size1 + bid_size2\r\n",
                "        - size_total_ask: mean of ask_size1 + ask_size2\r\n",
                "        - size_total1: mean of bid_size1 + ask_size1\r\n",
                "        - size_total2: mean of bid_size2 + ask_size2\r\n",
                "        - size_total3: mean of bid_size1 + bid_size2 + ask_size1 + ask_size2\r\n",
                "        - size_spread1: mean of bid_size1 - ask_size1\r\n",
                "        - size_spread2: mean of bid_size2 - ask_size2\r\n",
                "        - size_spread3: mean of bid_size1 + bid_size2 - ask_size1 - ask_size2\r\n",
                "    '''"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'\\nfeatures\\n========\\n    - stock_id: [categorical]\\n    - past_vol: volatility during the feature bucket of each (stock_id, time_id)\\n    - past_vol2: past volatility calculated with WAP2\\n    - updown: Compare average WAP of first 10 seconds_in_bucket and last 10 in the feature bucket.\\n              +1 if the first 10 < the last 10, -1 otherwise.\\n    - price_spread1: mean of ask_price1 - bid_price1\\n    - price_spread2: mean of ask_price2 - bid_price2\\n    - price_spread_bid: mean of bid_price1 - bid_price2\\n    - price_spread_ask: mean of ask_price1 - ask_price2\\n    - size_total_bid: mean of bid_size1 + bid_size2\\n    - size_total_ask: mean of ask_size1 + ask_size2\\n    - size_total1: mean of bid_size1 + ask_size1\\n    - size_total2: mean of bid_size2 + ask_size2\\n    - size_total3: mean of bid_size1 + bid_size2 + ask_size1 + ask_size2\\n    - size_spread1: mean of bid_size1 - ask_size1\\n    - size_spread2: mean of bid_size2 - ask_size2\\n    - size_spread3: mean of bid_size1 + bid_size2 - ask_size1 - ask_size2\\n'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "DATA_DIR = '../input/optiver-realized-volatility-prediction/'\r\n",
                "\r\n",
                "def read_book(train_test: str, stock_id: int):\r\n",
                "    if stock_id is None:\r\n",
                "        return pd.concat(read_book(train_test, i) for i in range(127))\r\n",
                "\r\n",
                "    book = pd.read_parquet(DATA_DIR + f\"book_{train_test}.parquet/stock_id={stock_id}\")\r\n",
                "    book[\"stock_id\"] = stock_id\r\n",
                "    return book"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "# Functions to compute features\r\n",
                "\r\n",
                "\r\n",
                "def wap(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return (\r\n",
                "        book[\"bid_price1\"] * book[\"ask_size1\"] + book[\"ask_price1\"] * book[\"bid_size1\"]\r\n",
                "    ) / (book[\"bid_size1\"] + book[\"ask_size1\"])\r\n",
                "\r\n",
                "\r\n",
                "def wap2(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return (\r\n",
                "        book[\"bid_price2\"] * book[\"ask_size2\"] + book[\"ask_price2\"] * book[\"bid_size2\"]\r\n",
                "    ) / (book[\"bid_size2\"] + book[\"ask_size2\"])\r\n",
                "\r\n",
                "\r\n",
                "def price_spread1(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book[\"ask_price1\"] - book[\"bid_price1\"]\r\n",
                "\r\n",
                "\r\n",
                "def price_spread2(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book[\"ask_price2\"] - book[\"bid_price2\"]\r\n",
                "\r\n",
                "\r\n",
                "def price_spread_bid(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book[\"bid_price1\"] - book[\"bid_price2\"]\r\n",
                "\r\n",
                "\r\n",
                "def price_spread_ask(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book[\"ask_price1\"] - book[\"ask_price2\"]\r\n",
                "\r\n",
                "\r\n",
                "def size_total_bid(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size1'] + book['bid_size2']\r\n",
                "\r\n",
                "\r\n",
                "def size_total_ask(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['ask_size1'] + book['ask_size2']\r\n",
                "\r\n",
                "\r\n",
                "def size_total1(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size1'] + book['ask_size1']\r\n",
                "\r\n",
                "\r\n",
                "def size_total2(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size2'] + book['ask_size2']\r\n",
                "\r\n",
                "\r\n",
                "def size_total3(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size1'] + book['bid_size2'] + book['ask_size1'] + book['ask_size2']\r\n",
                "\r\n",
                "\r\n",
                "def size_spread1(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size1'] - book['ask_size1']\r\n",
                "\r\n",
                "\r\n",
                "def size_spread2(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size2'] - book['ask_size2']\r\n",
                "\r\n",
                "\r\n",
                "def size_spread3(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book['bid_size1'] + book['bid_size2'] - book['ask_size1'] - book['ask_size2']\r\n",
                "\r\n",
                "\r\n",
                "def log_return(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return wap(book).groupby(book.time_id).apply(np.log).diff()\r\n",
                "\r\n",
                "\r\n",
                "def log_return2(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return wap2(book).groupby(book.time_id).apply(np.log).diff()\r\n",
                "\r\n",
                "\r\n",
                "def instvar(book: pd.DataFrame):\r\n",
                "    \"\"\"Compute instantaneous variance.\"\"\"\r\n",
                "    return log_return(book) ** 2\r\n",
                "\r\n",
                "\r\n",
                "def instvar2(book: pd.DataFrame):\r\n",
                "    \"\"\"Compute instantaneous variance.\"\"\"\r\n",
                "    return log_return2(book) ** 2\r\n",
                "\r\n",
                "\r\n",
                "def updown(book: pd.DataFrame) -> pd.Series:\r\n",
                "    n_samples = 10\r\n",
                "    wap_groupby = wap(book).groupby(book.time_id)\r\n",
                "    head = wap_groupby.head(n_samples).groupby(book.time_id).mean()\r\n",
                "    tail = wap_groupby.tail(n_samples).groupby(book.time_id).mean()\r\n",
                "    return (tail - head).apply(np.sign)\r\n",
                "\r\n",
                "\r\n",
                "def stock_id(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book.stock_id.groupby(book.time_id).first()\r\n",
                "\r\n",
                "\r\n",
                "def time_id(book: pd.DataFrame) -> pd.Series:\r\n",
                "    return book.time_id.groupby(book.time_id).first()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from functools import partial\r\n",
                "\r\n",
                "\r\n",
                "def derive(fn: Callable, name=None, **kwargs) -> Callable:\r\n",
                "    \"\"\"Derive a function,named name,which when called will behave like\r\n",
                "    fn called with the keyword arguments kwargs.\r\n",
                "    \"\"\"\r\n",
                "    function = partial(fn, **kwargs)\r\n",
                "    if name is not None:\r\n",
                "        function.__name__ = name\r\n",
                "    return function"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# Aggregation Functions\r\n",
                "\r\n",
                "\r\n",
                "def ewm(series: pd.Series, halflife: float = 10.0) -> pd.Series:\r\n",
                "    return series.ewm(halflife=halflife).mean().iat[-1]\r\n",
                "\r\n",
                "\r\n",
                "ewm10 = derive(ewm, \"ewm10\", halflife=10.0)\r\n",
                "ewm20 = derive(ewm, \"ewm20\", halflife=20.0)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "from collections import OrderedDict\r\n",
                "\r\n",
                "\r\n",
                "class CreateFeatures:\r\n",
                "    \"\"\"Class to compute features.\r\n",
                "\r\n",
                "    Args:\r\n",
                "        features: list[dict[str, (Callable, Callable)]]\r\n",
                "            Each key is the name of a feature.\r\n",
                "            Each value is a tuple: ``(fn, agg)`` where fn is a function applied to the book \r\n",
                "            to make series and agg is an aggregation function\r\n",
                "\r\n",
                "            If agg is not None:\r\n",
                "                feature (pd.Series) will be computed by 1) applying fn to a book,\r\n",
                "                2) aggregating the output by `time_id`, and then \r\n",
                "                3) aggregate the groupby object with agg.\r\n",
                "                fn receives pd.DataFrame of book and returns pd.Series of feature time-series.\r\n",
                "                agg is any aggregation function that can be passed to pd.SeriesGroupby.agg.\r\n",
                "\r\n",
                "            If agg is None:\r\n",
                "                feature will be computed by fn(book).\r\n",
                "    \"\"\"\r\n",
                "\r\n",
                "    def __init__(self, features: List[Tuple[Callable, Callable]]) -> None:\r\n",
                "        # attribute _features is:\r\n",
                "        # each key: name of feature \"{fn name}__{aggregation name}\"\r\n",
                "        # each value: (fn, agg) \r\n",
                "        self._features = OrderedDict()\r\n",
                "        for fn, agg in features:\r\n",
                "            name = self._feature_name(fn, agg)\r\n",
                "            self.register_feature(name, fn=fn, agg=agg)\r\n",
                "\r\n",
                "    def compute_df_features(self, book) -> pd.DataFrame:\r\n",
                "        return pd.DataFrame(\r\n",
                "            {\r\n",
                "                name: self._compute_feature(book, fn, agg)\r\n",
                "                for name, fn, agg in self.features()\r\n",
                "            }\r\n",
                "        )\r\n",
                "\r\n",
                "    def register_feature(self, name, fn, agg) -> None:\r\n",
                "        \"\"\"Adds a feature to the class.\"\"\"\r\n",
                "        self._features[name] = (fn, agg)\r\n",
                "\r\n",
                "    def features(self) -> Iterable:\r\n",
                "        \"\"\"Returns an iterator over (name, fn, agg) of features.\"\"\"\r\n",
                "        for name, (fn, agg) in self._features.items():\r\n",
                "            yield (name, fn, agg)\r\n",
                "\r\n",
                "    @property\r\n",
                "    def feature_names(self) -> List[str]:\r\n",
                "        return list(self._features.keys())\r\n",
                "\r\n",
                "    def _feature_name(self, fn, agg) -> str:\r\n",
                "        name = fn.__name__\r\n",
                "        if agg is not None:\r\n",
                "            name += \"__\" + (agg if isinstance(agg, str) else agg.__name__)\r\n",
                "        return name\r\n",
                "\r\n",
                "    def _compute_feature(self, book, fn, agg) -> pd.Series:\r\n",
                "        output = fn(book)\r\n",
                "        if agg is not None:\r\n",
                "            output = output.groupby(book.time_id).agg(agg)\r\n",
                "        return output"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "from itertools import product\r\n",
                "\r\n",
                "features = [\r\n",
                "    (stock_id, None),\r\n",
                "    (time_id, None),\r\n",
                "    (instvar, np.mean),  # volatility\r\n",
                "    (instvar2, np.mean),  # volatility\r\n",
                "    (updown, None),\r\n",
                "]\r\n",
                "\r\n",
                "fns = [price_spread1, price_spread2, price_spread_bid, price_spread_ask, size_total_bid, size_total_ask,\r\n",
                "       size_total1, size_total2, size_total3, size_spread1, size_spread2, size_spread3]\r\n",
                "aggs = [np.mean, ewm10, ewm20]\r\n",
                "for fn, agg in product(fns, aggs):\r\n",
                "    features.append((fn, agg))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "cf = CreateFeatures(features)\r\n",
                "\r\n",
                "cf.feature_names"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['stock_id',\n",
                            " 'time_id',\n",
                            " 'instvar__mean',\n",
                            " 'updown',\n",
                            " 'price_spread1__mean',\n",
                            " 'price_spread1__ewm10',\n",
                            " 'price_spread1__ewm20',\n",
                            " 'price_spread2__mean',\n",
                            " 'price_spread2__ewm10',\n",
                            " 'price_spread2__ewm20',\n",
                            " 'price_spread_bid__mean',\n",
                            " 'price_spread_bid__ewm10',\n",
                            " 'price_spread_bid__ewm20',\n",
                            " 'price_spread_ask__mean',\n",
                            " 'price_spread_ask__ewm10',\n",
                            " 'price_spread_ask__ewm20',\n",
                            " 'size_total_bid__mean',\n",
                            " 'size_total_bid__ewm10',\n",
                            " 'size_total_bid__ewm20',\n",
                            " 'size_total_ask__mean',\n",
                            " 'size_total_ask__ewm10',\n",
                            " 'size_total_ask__ewm20',\n",
                            " 'size_total1__mean',\n",
                            " 'size_total1__ewm10',\n",
                            " 'size_total1__ewm20',\n",
                            " 'size_total2__mean',\n",
                            " 'size_total2__ewm10',\n",
                            " 'size_total2__ewm20',\n",
                            " 'size_total3__mean',\n",
                            " 'size_total3__ewm10',\n",
                            " 'size_total3__ewm20',\n",
                            " 'size_spread1__mean',\n",
                            " 'size_spread1__ewm10',\n",
                            " 'size_spread1__ewm20',\n",
                            " 'size_spread2__mean',\n",
                            " 'size_spread2__ewm10',\n",
                            " 'size_spread2__ewm20',\n",
                            " 'size_spread3__mean',\n",
                            " 'size_spread3__ewm10',\n",
                            " 'size_spread3__ewm20']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 12
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "def create_df_features(cf: CreateFeatures, train_test: str = None) -> pd.DataFrame:\r\n",
                "    paths = glob.glob(DATA_DIR + f\"book_{train_test}.parquet/*\")\r\n",
                "    dfs = []\r\n",
                "    for path in tqdm(paths):\r\n",
                "        book = pd.read_parquet(path)\r\n",
                "        book[\"stock_id\"] = int(path.split(\"=\")[-1])\r\n",
                "        dfs.append(cf.compute_df_features(book))\r\n",
                "    df_features = pd.concat(dfs).reset_index(drop=True)\r\n",
                "    return df_features"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "class Model(metaclass=ABCMeta):\r\n",
                "\r\n",
                "    def __init__(self, run_fold_name: str, feature_names: List[str],\r\n",
                "                 params: dict) -> None:\r\n",
                "        ''' Constructor\r\n",
                "\r\n",
                "        :param feature_names: list of feature names to specify columns of feature dataframe\r\n",
                "        :param params: hyper parameters\r\n",
                "        '''\r\n",
                "        self.run_fold_name = run_fold_name\r\n",
                "        self.feature_names = feature_names\r\n",
                "        self.params = params\r\n",
                "        self.model = None\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def train(self, X_train: pd.DataFrame, y_train: pd.Series,\r\n",
                "              X_valid: Optional[pd.DataFrame] = None,\r\n",
                "              y_valid: Optional[pd.Series] = None\r\n",
                "              ) -> None:\r\n",
                "        ''' trains a model\r\n",
                "\r\n",
                "        :param X_train: features of training data\r\n",
                "        :param y_train: targets of training data\r\n",
                "        :param X_valid: features of validation data\r\n",
                "        :param y_valid: targets of validation data\r\n",
                "        '''\r\n",
                "        pass\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def predict(self, X: pd.DataFrame) -> np.array:\r\n",
                "        ''' returns prediction output from a learned model\r\n",
                "\r\n",
                "        :param X: features of test data or validation data\r\n",
                "        :return: predicted value\r\n",
                "        '''\r\n",
                "        pass\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def save_model(self) -> None:\r\n",
                "        ''' saves a model '''\r\n",
                "        pass\r\n",
                "\r\n",
                "    @abstractmethod\r\n",
                "    def load_model(self) -> None:\r\n",
                "        ''' loads a model '''\r\n",
                "        pass"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "class ModelLGB(Model):\r\n",
                "\r\n",
                "\r\n",
                "    def train(self, X_train, y_train, X_valid=None, y_valid=None):\r\n",
                "\r\n",
                "        params = self.params\r\n",
                "        if X_valid is not None:\r\n",
                "            # weights to change feval from RMSE to RMSPE\r\n",
                "            # idea from: https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/250324\r\n",
                "            w_train = 1/np.square(y_train)\r\n",
                "            w_valid = 1/np.square(y_valid)\r\n",
                "            lgb_train = lgb.Dataset(X_train, y_train, weight=w_train)\r\n",
                "            lgb_eval = lgb.Dataset(X_valid, y_valid, weight=w_valid)\r\n",
                "            self.model = lgb.train(params,\r\n",
                "                                   lgb_train,\r\n",
                "                                   num_boost_round=100,\r\n",
                "                                   valid_sets=lgb_eval,\r\n",
                "                                   feval=rmspe,\r\n",
                "                                   verbose_eval=50,\r\n",
                "                                   categorical_feature=['stock_id']\r\n",
                "                                  )\r\n",
                "        else:\r\n",
                "            w_train = 1/np.square(y_train)\r\n",
                "            lgb_train = lgb.Dataset(X_train, y_train, weight=w_train)\r\n",
                "            self.model = lgb.train(params,\r\n",
                "                                   lgb_train,\r\n",
                "                                   num_boost_round=100,\r\n",
                "                                   feval=rmspe,\r\n",
                "                                   categorical_feature=['stock_id']\r\n",
                "                           )\r\n",
                "\r\n",
                "    def predict(self, X_test):\r\n",
                "        return self.model.predict(X_test)\r\n",
                "\r\n",
                "    def save_model(self):\r\n",
                "        self.model.save_model('model.txt')\r\n",
                "\r\n",
                "    def load_model(self):\r\n",
                "        self.model = lgb.Booster(model_file='model.txt')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "# delf-defiend evaluation metric function\r\n",
                "def rmspe(y_true: np.array, y_pred: np.array):\r\n",
                "    ''' self-defined eval metric\r\n",
                "        Root Mean Squared Percentage Error\r\n",
                "\r\n",
                "    :return: name: str, eval_result: float, is_higher_better: bool\r\n",
                "    '''\r\n",
                "    if type(y_pred) == lgb.basic.Dataset:\r\n",
                "        y_pred = y_pred.get_label()\r\n",
                "    rmspe = (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\r\n",
                "    return 'RMSPE', rmspe, False"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "class Util:\r\n",
                "    @classmethod\r\n",
                "    def dump(cls, value, path):\r\n",
                "        os.makedirs(os.path.dirname(path), exist_ok=True)\r\n",
                "        joblib.dump(value, path, compress=True)\r\n",
                "\r\n",
                "    @classmethod\r\n",
                "    def load(cls, path):\r\n",
                "        return joblib.load(path)\r\n",
                "\r\n",
                "    @classmethod\r\n",
                "    def submission(cls, df_pred: pd.DataFrame) -> None:\r\n",
                "        row_id = df_pred['stock_id'].apply(str) + '-' + df_pred['time_id'].apply(str)\r\n",
                "        df_submission = pd.DataFrame({'row_id': row_id, 'target':df_pred['target']})\r\n",
                "        df_submission.to_csv('submission.csv', index=False)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "class Runner:\r\n",
                "\r\n",
                "    def __init__(self, run_name: str, model_cls: Callable[[str, dict], Model],\r\n",
                "                 feature_names: List[str], df_features_train: pd.DataFrame,\r\n",
                "                 df_features_test: pd.DataFrame, params: dict):\r\n",
                "        ''' Constructor\r\n",
                "\r\n",
                "        :param run_name: name of run\r\n",
                "        :param model_cls: class of model\r\n",
                "        :param feature_names: list of feature names\r\n",
                "        :param params: hyper parameters\r\n",
                "        '''\r\n",
                "        self.run_name = run_name\r\n",
                "        self.model_cls = model_cls\r\n",
                "        self.feature_names = feature_names\r\n",
                "        self.X_train_all = df_features_train\r\n",
                "        self.X_test = df_features_test\r\n",
                "        self.params = params\r\n",
                "        self.n_fold = 4\r\n",
                "\r\n",
                "    def _train_fold(self, i_fold: Union[int, str]) -> Tuple[\r\n",
                "                    Model, Optional[np.array],\r\n",
                "                    Optional[np.array], Optional[float]]:\r\n",
                "        ''' specifies number of fold for cv then learns & evaluates\r\n",
                "\r\n",
                "        :param i_fold: number of fold ('all' for all)\r\n",
                "        :return: a tuple of instance of model, index of record,\r\n",
                "                 predicted value, and evaluation score\r\n",
                "                 (returns only model if i_fold=='all')\r\n",
                "        '''\r\n",
                "        # load train data\r\n",
                "        y_train_all = self._load_y_train()\r\n",
                "        self.X_train_all['stock_id'] = self.X_train_all['stock_id'].astype('int64')\r\n",
                "        self.X_train_all['time_id'] = self.X_train_all['time_id'].astype('int64')\r\n",
                "        Xy_train_all = pd.merge(self.X_train_all, y_train_all,\r\n",
                "                                left_on=['stock_id','time_id'], right_on=['stock_id','time_id'],\r\n",
                "                                how='inner')\r\n",
                "        X_train_all = Xy_train_all[self.feature_names]\r\n",
                "        y_train_all = Xy_train_all['target']\r\n",
                "\r\n",
                "        validation = i_fold != 'all'\r\n",
                "        if validation:\r\n",
                "            # split data into training and validation\r\n",
                "            idx_train, idx_valid = self._load_index_fold(i_fold)\r\n",
                "            X_train = X_train_all.iloc[idx_train]\r\n",
                "            y_train = y_train_all.iloc[idx_train]\r\n",
                "            X_valid = X_train_all.iloc[idx_valid]\r\n",
                "            y_valid = y_train_all.iloc[idx_valid]\r\n",
                "\r\n",
                "            # execute learning\r\n",
                "            model = self._build_model(i_fold)\r\n",
                "            model.train(X_train, y_train, X_valid, y_valid)\r\n",
                "\r\n",
                "            # prediction and evaluation with validation data\r\n",
                "            pred_valid = model.predict(X_valid)\r\n",
                "            _, score, _ = rmspe(y_true=y_valid, y_pred=pred_valid)\r\n",
                "\r\n",
                "            # return model, index, prediction, and score\r\n",
                "            return model, idx_valid, pred_valid, score\r\n",
                "        else:\r\n",
                "            # learining with all data\r\n",
                "            model = self._build_model(i_fold)\r\n",
                "            model.train(X_train_all, y_train_all)\r\n",
                "\r\n",
                "            # return model\r\n",
                "            return model, None, None, None\r\n",
                "\r\n",
                "    def run_train_cv(self) -> None:\r\n",
                "        ''' learns and evaluates by CV\r\n",
                "\r\n",
                "        learns, evaluates, and saves models and scores of each fold\r\n",
                "        '''\r\n",
                "        scores = []\r\n",
                "        idxes_valid = []\r\n",
                "        preds = []\r\n",
                "\r\n",
                "        # learning for each fold\r\n",
                "        for i_fold in range(self.n_fold):\r\n",
                "            model, idx_valid, pred_valid, score = self._train_fold(i_fold)\r\n",
                "\r\n",
                "            # hold result\r\n",
                "            idxes_valid.append(idx_valid)\r\n",
                "            scores.append(score)\r\n",
                "            preds.append(pred_valid)\r\n",
                "\r\n",
                "            # save model\r\n",
                "            model.save_model()\r\n",
                "        print(f'Mean score of the folds: {np.mean(scores)}')\r\n",
                "\r\n",
                "    def run_predict_cv(self) -> pd.DataFrame:\r\n",
                "        ''' predicts for test data with the mean of\r\n",
                "            each fold's model learned through CV\r\n",
                "            \r\n",
                "            :return: predicted target as the mean of folds\r\n",
                "        '''\r\n",
                "        preds = []\r\n",
                "        # prediction for each fold's model\r\n",
                "        for i_fold in range(self.n_fold):\r\n",
                "            model = self._build_model(i_fold)\r\n",
                "            model.load_model()\r\n",
                "            pred = model.predict(self.X_test[self.feature_names])\r\n",
                "            preds.append(pred)\r\n",
                "\r\n",
                "        # mean of the prediction values\r\n",
                "        pred_mean = np.mean(preds, axis=0)\r\n",
                "        df_pred = self.X_test[self.X_test.columns[:2]]\r\n",
                "        df_pred.loc[:, 'target'] = pred_mean\r\n",
                "        return df_pred\r\n",
                "\r\n",
                "    def run_train_all(self) -> None:\r\n",
                "        ''' learns with all the training data and save the model '''\r\n",
                "        # learning\r\n",
                "        i_fold = 'all'\r\n",
                "        model, _, _, _ = self._train_fold(i_fold)\r\n",
                "        model.save_model()\r\n",
                "\r\n",
                "    def run_predict_all(self) -> pd.DataFrame:\r\n",
                "        ''' predicts for test data with the model learned with all the training data\r\n",
                "\r\n",
                "        :return: predicted target\r\n",
                "        '''\r\n",
                "        \r\n",
                "        # predict with the mdoel learned with all the learning data\r\n",
                "        i_fold = 'all'\r\n",
                "        model = self._build_model(i_fold)\r\n",
                "        model.load_model()\r\n",
                "        pred = model.predict(self.X_test[self.feature_names])\r\n",
                "        df_pred = self.X_test[self.X_test.columns[:2]]\r\n",
                "        df_pred['target'] = pred\r\n",
                "        return df_pred\r\n",
                "\r\n",
                "    def _build_model(self, i_fold: Union[int, str]) -> Model:\r\n",
                "        ''' builds a model with a specified fold for cv\r\n",
                "\r\n",
                "        :param i_fold: number of fold\r\n",
                "        :return: instance of model\r\n",
                "        '''\r\n",
                "        # build a model with run name, fold, and class of model\r\n",
                "        run_fold_name = f'{self.run_name}-{i_fold}'\r\n",
                "        return self.model_cls(run_fold_name, self.feature_names, self.params)\r\n",
                "\r\n",
                "    def _load_y_train(self) -> pd.DataFrame:\r\n",
                "        ''' loads target of train data; ['stock_id', 'time_id', 'target']\r\n",
                "\r\n",
                "        :return: target dataframe of train data\r\n",
                "        '''\r\n",
                "        return pd.read_csv(DATA_DIR + 'train.csv')\r\n",
                "\r\n",
                "    def _load_index_fold(self, i_fold: int) -> np.array:\r\n",
                "        ''' returns the record index in response to the fold specified for cv\r\n",
                "\r\n",
                "        :param i_fold: number of the fold\r\n",
                "        :return: record index for the fold\r\n",
                "        '''\r\n",
                "        # return index to split data for learning and validation\r\n",
                "        y_train = self._load_y_train()\r\n",
                "        x_dummy = np.zeros(len(y_train))\r\n",
                "        skf = KFold(n_splits=self.n_fold, shuffle=True, random_state=31)\r\n",
                "        return list(skf.split(x_dummy, y_train))[i_fold]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## main"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "params_lgb = {\r\n",
                "            'boosting_type': 'gbdt',\r\n",
                "            'num_leaves': 100,\r\n",
                "            'learning_rate': 0.05,\r\n",
                "            'n_estimators': 1000,\r\n",
                "            'metric': 'rmse',\r\n",
                "            'early_stopping_rounds': 100\r\n",
                "            }\r\n",
                "\r\n",
                "# rerun=False if just using saved features works\r\n",
                "# cf = CreateFeatures(rerun=True)\r\n",
                "df_features_train = create_df_features(cf, \"train\")\r\n",
                "df_features_test = create_df_features(cf, \"test\")\r\n",
                "feature_names = cf.feature_names\r\n",
                "\r\n",
                "# learning and prediction by lightGBM and create submission file\r\n",
                "run_name = 'lgb'\r\n",
                "runner = Runner(run_name=run_name,\r\n",
                "                model_cls=ModelLGB,\r\n",
                "                feature_names=feature_names,\r\n",
                "                df_features_train=df_features_train,\r\n",
                "                df_features_test=df_features_test,\r\n",
                "                params=params_lgb)\r\n",
                "runner.run_train_cv()\r\n",
                "pred = runner.run_predict_cv()\r\n",
                "Util.submission(pred)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 112/112 [30:43<00:00, 16.46s/it]\n",
                        "100%|██████████| 1/1 [00:00<00:00, 14.71it/s]\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065216 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 8275\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 34\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001803\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000679077\tvalid_0's RMSPE: 0.532298\n",
                        "[100]\tvalid_0's rmse: 0.000663166\tvalid_0's RMSPE: 0.496104\n",
                        "[150]\tvalid_0's rmse: 0.000661428\tvalid_0's RMSPE: 0.484754\n",
                        "[200]\tvalid_0's rmse: 0.000660546\tvalid_0's RMSPE: 0.478092\n",
                        "[250]\tvalid_0's rmse: 0.000661233\tvalid_0's RMSPE: 0.473811\n",
                        "Early stopping, best iteration is:\n",
                        "[180]\tvalid_0's rmse: 0.000660332\tvalid_0's RMSPE: 0.480888\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066174 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 8275\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 34\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001801\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000668736\tvalid_0's RMSPE: 0.524792\n",
                        "[100]\tvalid_0's rmse: 0.000651945\tvalid_0's RMSPE: 0.488001\n",
                        "[150]\tvalid_0's rmse: 0.000649391\tvalid_0's RMSPE: 0.476355\n",
                        "[200]\tvalid_0's rmse: 0.000648568\tvalid_0's RMSPE: 0.469881\n",
                        "[250]\tvalid_0's rmse: 0.000648165\tvalid_0's RMSPE: 0.465092\n",
                        "[300]\tvalid_0's rmse: 0.000647915\tvalid_0's RMSPE: 0.460946\n",
                        "[350]\tvalid_0's rmse: 0.000647666\tvalid_0's RMSPE: 0.457936\n",
                        "[400]\tvalid_0's rmse: 0.000647602\tvalid_0's RMSPE: 0.455122\n",
                        "[450]\tvalid_0's rmse: 0.000648017\tvalid_0's RMSPE: 0.452501\n",
                        "Early stopping, best iteration is:\n",
                        "[384]\tvalid_0's rmse: 0.000647438\tvalid_0's RMSPE: 0.455957\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089413 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 8275\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 34\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001800\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000673829\tvalid_0's RMSPE: 0.53109\n",
                        "[100]\tvalid_0's rmse: 0.000659224\tvalid_0's RMSPE: 0.494341\n",
                        "[150]\tvalid_0's rmse: 0.000656648\tvalid_0's RMSPE: 0.483187\n",
                        "[200]\tvalid_0's rmse: 0.000655463\tvalid_0's RMSPE: 0.476361\n",
                        "[250]\tvalid_0's rmse: 0.000655578\tvalid_0's RMSPE: 0.472052\n",
                        "[300]\tvalid_0's rmse: 0.000655508\tvalid_0's RMSPE: 0.468304\n",
                        "Early stopping, best iteration is:\n",
                        "[229]\tvalid_0's rmse: 0.00065503\tvalid_0's RMSPE: 0.473946\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\engine.py:156: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
                        "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
                        "New categorical_feature is ['stock_id']\n",
                        "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069351 seconds.\n",
                        "You can set `force_col_wise=true` to remove the overhead.\n",
                        "[LightGBM] [Info] Total Bins 8275\n",
                        "[LightGBM] [Info] Number of data points in the train set: 321699, number of used features: 34\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1286: UserWarning: Overriding the parameters from Reference Dataset.\n",
                        "  warnings.warn('Overriding the parameters from Reference Dataset.')\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\lightgbm\\basic.py:1098: UserWarning: categorical_column in param dict is overridden.\n",
                        "  warnings.warn('{} in param dict is overridden.'.format(cat_alias))\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[LightGBM] [Info] Start training from score 0.001797\n",
                        "Training until validation scores don't improve for 100 rounds\n",
                        "[50]\tvalid_0's rmse: 0.000677387\tvalid_0's RMSPE: 0.533656\n",
                        "[100]\tvalid_0's rmse: 0.000660441\tvalid_0's RMSPE: 0.49648\n",
                        "[150]\tvalid_0's rmse: 0.000655875\tvalid_0's RMSPE: 0.483563\n",
                        "[200]\tvalid_0's rmse: 0.000654014\tvalid_0's RMSPE: 0.477053\n",
                        "[250]\tvalid_0's rmse: 0.000652621\tvalid_0's RMSPE: 0.472403\n",
                        "[300]\tvalid_0's rmse: 0.000651924\tvalid_0's RMSPE: 0.468673\n",
                        "[350]\tvalid_0's rmse: 0.000651329\tvalid_0's RMSPE: 0.466053\n",
                        "[400]\tvalid_0's rmse: 0.000651315\tvalid_0's RMSPE: 0.463245\n",
                        "[450]\tvalid_0's rmse: 0.000651098\tvalid_0's RMSPE: 0.461104\n",
                        "[500]\tvalid_0's rmse: 0.00065108\tvalid_0's RMSPE: 0.458449\n",
                        "[550]\tvalid_0's rmse: 0.00065081\tvalid_0's RMSPE: 0.456286\n",
                        "[600]\tvalid_0's rmse: 0.000651048\tvalid_0's RMSPE: 0.454561\n",
                        "[650]\tvalid_0's rmse: 0.000651326\tvalid_0's RMSPE: 0.452653\n",
                        "Early stopping, best iteration is:\n",
                        "[563]\tvalid_0's rmse: 0.000650783\tvalid_0's RMSPE: 0.455796\n",
                        "Mean score of the folds: 0.3024017266376019\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  self.obj[key] = value\n",
                        "C:\\Users\\kotamya\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  self._setitem_single_column(ilocs[0], value, pi)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('mlenv': conda)"
        },
        "interpreter": {
            "hash": "708ee50cc1f420053506a448dda1816f977733ceebd2b39e67d8a3a227913ecf"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}